import numpy as np
import pandas as pd

from scipy.stats import entropy
from scipy.stats import shapiro, kruskal, mannwhitneyu



import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator

from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
import plotly.graph_objects as go

from sklearn.manifold import TSNE, trustworthiness
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import f_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Descriptors
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator

from transformers import AutoTokenizer, AutoModel
import torch
import umap



def generate_fingerprints(smiles_list, radius=3, n_bits=1024):
    """
    Generates Morgan fingerprints for a list of SMILES strings.

    Morgan fingerprints are a type of circular fingerprint used in cheminformatics 
    to represent molecular structures. They are generated by considering the 
    neighborhood of atoms within a given radius and creating a binary or integer 
    vector that encodes the presence or absence of specific substructures. The 
    radius defines the size of the neighborhood, and the number of bits determines 
    the length of the resulting fingerprint vector.

    Parameters:
    smiles_list (list of str): List of SMILES strings for which to generate fingerprints.
    radius (int, optional): Radius of the circular substructures used in the Morgan fingerprint algorithm. Default is 2.
    n_bits (int, optional): Number of bits in the fingerprint vector. Default is 1024.

    Returns:
    numpy.ndarray: Array of fingerprint vectors, where each vector corresponds to a SMILES string. 
                   Entries with invalid SMILES strings are omitted.
    """
    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)
    fingerprints = []
    
    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)
        if mol:
            fp = generator.GetFingerprint(mol)
            arr = np.zeros((1,), dtype=int)
            AllChem.DataStructs.ConvertToNumpyArray(fp, arr)
            fingerprints.append(arr)
        else:
            fingerprints.append(None)
    
    return np.array([fp for fp in fingerprints if fp is not None])


def get_BERT_model(model_name):
    """
    Loads a pre-trained BERT model and its corresponding tokenizer.

    BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based
    model developed by Google for natural language understanding tasks. It is pre-trained on a 
    large corpus of text using a masked language modeling task, allowing it to capture bidirectional 
    context in a sentence. BERT can be fine-tuned for specific downstream tasks, such as text classification, 
    named entity recognition, question answering, and more.

    This function loads both the pre-trained BERT model and its tokenizer from the Hugging Face 
    Model Hub using the provided model name. The tokenizer is used to convert input text into a format 
    that the BERT model can understand, while the model is used to generate embeddings or predictions 
    based on input text.

    Parameters:
    model_name (str): The name of the pre-trained BERT model to load (e.g., 'bert-base-uncased', 'bert-large-uncased').

    Returns:
    model (transformers.PreTrainedModel): The pre-trained BERT model.
    tokenizer (transformers.PreTrainedTokenizer): The corresponding tokenizer for the BERT model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    return model, tokenizer



def plot_elbow_curve(
    method, param1_list, param2_list, Ks, 
    original_data, param1_name='n_neighbors', 
    param2_name='min_dist', metric='euclidean',generate_MF=False):
    """
    Function to plot elbow curves of dimension reduced data, for clustering analysis on embeddings.

    Parameters:
        method (str): Dimensionality reduction method ('umap' or 'tsne').
        original_data (array): Original embeddings for UMAP, SMILES list for t-SNE.
        param1_list (list): List of values for the first parameter (e.g., 'n_neighbors' or 'radius').
        param2_list (list): List of values for the second parameter (e.g., 'min_dist' or 'perplexity').
        Ks (range): Range of k values for KMeans clustering.
        param1_name (str): Name of the first parameter.
        param2_name (str): Name of the second parameter.
        metric (str): Distance metric to use.

    Returns:
        tuple: results (list), scores (array), silhouette_scores (list)
    """
    
    nrows, ncols = len(param1_list), len(param2_list)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4*ncols, 4*nrows))
    axes = np.atleast_2d(axes)
    sns.set_palette('deep', n_colors=nrows * ncols)
    
    results = [[[] for _ in range(ncols)] for _ in range(nrows)]
    scores = np.zeros((nrows, ncols))
    silhouette_scores = [[[] for _ in range(ncols)] for _ in range(nrows)]
    
    for idx1, param1 in enumerate(param1_list):
        print(f'{param1_name.capitalize()} = {param1}')
        
        if generate_MF:
            embeddings = generate_fingerprints(original_data, radius=param1)
        else:
            embeddings = original_data
        
        for idx2, param2 in enumerate(param2_list):
            print(f'{param2_name.capitalize()} = {param2}')
            ax = axes[idx1][idx2]
            
            if method == 'umap':
                model = umap.UMAP(n_components=3, n_neighbors=param1, min_dist=param2, metric=metric)
                result = model.fit_transform(embeddings)
                scores[idx1][idx2] = trustworthiness(embeddings, result, n_neighbors=param1)
                
            elif method == 'tsne':
                model = TSNE(n_components=3, perplexity=param2, init='random', learning_rate='auto', random_state=0, metric=metric)
                result = model.fit_transform(embeddings)
                scores[idx1][idx2] = model.kl_divergence_
            
            results[idx1][idx2][:] = result
            
            inertia = []
            s_score = []
            for k in Ks:
                kmeans = KMeans(n_clusters=k, random_state=42)
                kmeans.fit(result)
                inertia.append(kmeans.inertia_)
                s_score.append(silhouette_score(result, kmeans.labels_))
            silhouette_scores[idx1][idx2][:] = s_score
            
            ax.plot(Ks, inertia, 'o-', markersize=8, label=f'{param1_name}={param1}, {param2_name}={param2}', color=np.random.rand(3,))
            ax.xaxis.set_major_locator(MaxNLocator(integer=True))
            ax.legend()
            ax.set_xlabel('Number of Clusters (k)')
            ax.set_ylabel('Inertia')
            ax.grid(True)

    plt.suptitle(f'Elbow Method for Optimal k with varying {param1_name} and {param2_name}', fontweight='bold', fontsize=12)
    plt.tight_layout()
    plt.show()

    return results, scores, silhouette_scores


def plot_silhouette_scores(
    s_scores, Ks, param1_values, param2_values, param1_name, param2_name, 
    y_lim, color_palette="inferno", title="silhouette score for different k"
):
    """
    Function to plot silhouette scores for clustering analysis.

    Parameters:
        s_scores (list): A nested list of silhouette scores for each combination of parameters.
        Ks (range): Range of k values for KMeans clustering.
        param1_values (list): List of values for the first parameter (e.g., 'n_neighbors' or 'radius').
        param2_values (list): List of values for the second parameter (e.g., 'min_dist' or 'perplexity').
        param1_name (str): Name of the first parameter to display in the title (e.g., 'NN' or 'r').
        param2_name (str): Name of the second parameter to display in the legend (e.g., 'd' or 'p').
        y_lim (tuple): Y-axis limits for the silhouette scores.
        color_palette (str): Color palette for plotting.
        title (str): Title for the overall plot.
    """
    ncols = len(param1_values)
    fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=(4*ncols, 6))
    axes = np.atleast_2d(axes)
    axes = axes.flatten()
    palette = sns.color_palette(color_palette, n_colors=len(param2_values))
    
    for idx1, row in enumerate(s_scores):
        for idx2, s_score in enumerate(row):
            axes[idx1].plot(Ks, s_score, label=f'{param2_name}={param2_values[idx2]}', color=palette[idx2])
        axes[idx1].legend()
        axes[idx1].xaxis.set_major_locator(MaxNLocator(integer=True))
        axes[idx1].set_xlabel('number of clusters (k)')
        axes[idx1].set_ylabel('silhouette score')
        axes[idx1].set_ylim(y_lim)
        axes[idx1].set_title(f'{param1_name}={param1_values[idx1]}')
        axes[idx1].grid(True)

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()


def normalized_entropy(counts):
    """
    Calculates the normalized entropy of a discrete distribution.

    Entropy is a measure of uncertainty or unpredictability in a distribution.
    The normalized entropy scales the raw entropy value so that it lies between 0 and 1.
    A normalized entropy of 1 corresponds to the highest possible entropy (maximum uncertainty), 
    which occurs when the distribution is uniform (all categories are equally likely). 
    A normalized entropy of 0 corresponds to the lowest possible entropy (minimum uncertainty), 
    which occurs when all counts are concentrated in a single category.

    The raw entropy is calculated using the Shannon entropy formula, and then it is normalized 
    by dividing it by the maximum possible entropy, which for a discrete distribution is the 
    logarithm of the number of categories (k).

    Parameters:
    counts (array-like): Array of counts for each category in the distribution.
                          Each entry represents the number of occurrences of a particular category.

    Returns:
    float: Normalized entropy value between 0 and 1. 
           - 1 indicates maximum entropy (uniform distribution, highest uncertainty).
           - 0 indicates minimum entropy (distribution concentrated in a single category).
    """
    k = len(counts) 
    probs = counts / counts.sum()  
    raw_entropy = entropy(probs)   
    max_entropy = np.log(k)       
    return raw_entropy / max_entropy 



def plot_cluster_distributions(
    results, best_ks, param1_values, param2_values, param1_name, param2_name,
    palette='viridis', figsize=(25, 25), title="Clusters distribution"
):
    """
    Function to plot the distribution of clusters for different combinations of parameters.
    Uses normalized entropy as indicator metric.

    Parameters:
        results (list): List of dimensionality-reduced results for each combination of parameters.
        best_ks (list): List of best k values for each parameter combination.
        param1_values (list): List of values for the first parameter (e.g., radiuses or n_neighbors).
        param2_values (list): List of values for the second parameter (e.g., perplexities or min_dists).
        param1_name (str): Name of the first parameter to display in the title.
        param2_name (str): Name of the second parameter to display in the title.
        palette (str): Color palette for plotting.
        figsize (tuple): Figure size for the plot.
        title (str): Title for the overall plot.
    """
    nrows, ncols = len(param1_values), len(param2_values)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
    axes = np.atleast_2d(axes)
    
    for idx1, param1 in enumerate(param1_values):
        for idx2, param2 in enumerate(param2_values):
            ax = axes[idx1][idx2]
            kmeans = KMeans(n_clusters=best_ks[idx1][idx2], random_state=42)
            kmeans.fit(results[idx1][idx2])

            # Add cluster labels to the DataFrame
            labels = pd.DataFrame(kmeans.labels_, columns=['labels'])
            cluster_counts = labels['labels'].value_counts()
            entropy_score = round(normalized_entropy(cluster_counts), 4)

            sns.countplot(x='labels', hue='labels', data=labels, palette=palette, ax=ax)
            ax.set_title(f'({param1_name}={param1} | {param2_name}={param2} | entropy score={entropy_score})')
            ax.set_xlabel('Cluster')
            ax.set_ylabel('Number of Ligands')

            legend_labels = [f'{cluster} ({cluster_counts.get(cluster, 0)})' for cluster in range(best_ks[idx1][idx2])]
            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='grey', markersize=10) for _ in range(best_ks[idx1][idx2])]
            ax.legend(handles, legend_labels, title='Cluster')

    plt.suptitle(title, fontweight='bold', fontsize=18, y=1.001)
    plt.tight_layout()
    plt.show()


def plot_3d_clusters(results, min_index, k, method_name):
    """
    Function to create a 3D interactive scatter plot with clusters based on dimensionality reduction results.

    Parameters:
        results (list): List of dimensionality-reduced results for each parameter combination.
        min_index (tuple): Index of the optimal parameters in the results list.
        k (int): Number of clusters for KMeans.
        method_name (str): Name of the method (e.g., 'TSNE' or 'UMAP') for labeling axes.
    """
    # KMeans clustering on the selected result
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(results[min_index[0]][min_index[1]])
    df = pd.DataFrame(results[min_index[0]][min_index[1]], columns=[f'{method_name}1', f'{method_name}2', f'{method_name}3'])
    df['Cluster'] = kmeans.labels_

    # Create 3D scatter plot
    fig = go.Figure()
    scatter = go.Scatter3d(
        x=df[f'{method_name}1'],
        y=df[f'{method_name}2'],
        z=df[f'{method_name}3'],
        mode='markers',
        marker=dict(size=5, color=df['Cluster'], colorscale='Inferno', opacity=0.7)
    )
    fig.add_trace(scatter)

    # Customize layout
    fig.update_layout(
        title=f"{method_name} Clusters",
        scene=dict(
            xaxis_title=f'{method_name}1',
            yaxis_title=f'{method_name}2',
            zaxis_title=f'{method_name}3',
            xaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
            yaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
            zaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
        ),
        height=800, width=1200,
        template="plotly_white"
    )

    # Display the figure
    fig.show()

    return df

def shapiro_test(data, param, r = 4):
    '''
    function that tests if the distribution of the data is normal

    parameters:
    param(string): name of the column of the dataframe to be tested 
    '''
    print(f'Shapiro-Wilk test for the parameter: {param}')
    for i in range(4):
        stat, p_value = shapiro(data[f'Cluster_{i}'][param])

        print(f"Statistic: {stat}, p-value: {p_value}")

        # Interpret the result
        if p_value > 0.05:
            print("Data looks normally distributed (fail to reject H0).")
        else:
            print("Data does not look normally distributed (reject H0).")

def mann_whiteney_test(data1, data2,  sent):
    '''
    function that tests in a non-parametric way if the differece between the datasets is significant

    parameters:
    data1(Dataframe): first dataset to be tested
    data2(Dataframe): second dataset to be tested
    sent(string): sentnce describing the datasets tested
    '''
    print(f'Mann-Whiney U test')
    stat, p_value = mannwhitneyu(data1, data2, alternative='two-sided')
    print(f"Statistic: {stat}, p-value: {p_value}")

    if p_value < 0.05:
        print("Significant difference between", sent)
    else:
        print("No significant difference between", sent)

def test_kruskal(column_to_check, grouped_dataset):
    ''' 
    Perform a Kruskal-Wallis test on column_to_check
    grouped_dataset: dataset grouped by cluster
    '''
    print(f'Kruskal-Wallis test for {column_to_check}.')
    results = []
    for _, group in grouped_dataset:
        column_values = group[column_to_check].values
        results.append(column_values)
    stat, p = kruskal(*results)
    print(f'H-statistic: {stat}, p-value: {p}.')
    if p > 0.05:
        print(f'There is no evidence of a significant difference in medians accross the clusters for {column_to_check}.\n')
    else:
        print(f'There is a significant difference in the medians of at least one group compared to the other in {column_to_check}.\n')

def extract_info_smiles(data):
    '''
    function that extracts properties (h_bond acceptor, h_bond donnor, molecular weight, LogP) velues from the SMILES

    parameter:
    data(Dataframe): dataset of SMILES
    '''
    molecular_weights=[]
    h_bond_donors=[]
    h_bond_acceptors=[]
    logP=[]

    
    for smiles in data['Ligand SMILES']:
        try:
            mol = Chem.MolFromSmiles(smiles)  # Create RDKit molecule object
            if mol:
                # Calculate properties if SMILES is valid
                molecular_weights.append(Descriptors.MolWt(mol))
                h_bond_donors.append(Descriptors.NumHDonors(mol))
                h_bond_acceptors.append(Descriptors.NumHAcceptors(mol))
                logP.append(Descriptors.MolLogP(mol))
            else:
                # Add None if SMILES is invalid
                molecular_weights.append(None)
                h_bond_donors.append(None)
                h_bond_acceptors.append(None)
        except Exception as e:
            # Handle invalid SMILES error by appending None
            print(f"Error processing SMILES {smiles}: {e}")
            molecular_weights.append(None)
            h_bond_donors.append(None)
            h_bond_acceptors.append(None)

    # Add properties to DataFrame
    data['Molecular Weight'] = molecular_weights
    data['H-Bond Donors'] = h_bond_donors
    data['H-Bond Acceptors'] = h_bond_acceptors

    # Drop rows with any missing values in these new columns
    data.dropna(subset=['Molecular Weight', 'H-Bond Donors', 'H-Bond Acceptors'], inplace=True)


#Targets visualization per clusters
def clusters_targets(data, ax, title):
    '''
    function that create a barplot with top 20 targets of the dataset

    parameter: 
    data (dataframe): dataset with target names
    ax: axe of the figure 
    title(string)
    '''
    target_counts = data.groupby('Target Name').size().reset_index(name='Compound Count')
    target_counts = target_counts.sort_values(by='Compound Count', ascending=False)
    sns.barplot(x='Compound Count', y='Target Name', data=target_counts.head(20), palette='viridis', ax = ax)
    ax.set_title(title)
    ax.set_xlabel('Number of Compounds')
    ax.set_ylabel('Target Name')


def MW_histplot(data, ax, title, xlim=1500):
    '''
    function that plots a histogramm of the distribution of the molecular weight in the dataset

    parameter:
    data (Dataframe): dataset containing a column 'Molecular Weight'
    ax(array): position of the subplot in the figure
    title (string): title of the plot
    xlim: x values limit
    '''
    sns.histplot(data['Molecular Weight'], bins=30, color="skyblue", ax=ax)
    ax.set_title(title)
    ax.set_xlabel("Molecular Weight (g/mol)")
    ax.set_ylabel("Frequency")
    if xlim:
        ax.set_xlim(xlim)

# 
def LogP(data):
    '''
    function to visualize the distribution of hydrophobicity value LogP between clusters
    
    Parameter: 
    data(Dataframe): dataframe with LogP value and cluster column
    '''
    sns.boxplot(data = data, x= 'Cluster', y= 'logP', fill = False)
    medians = data.groupby('Cluster')['logP'].median()
    for i, median in enumerate(medians):
        plt.text(i, median + 0.1, f'{median:.2f}', color = 'black')

    plt.scatter([], [], color='yellow', label='Median value', marker='o')  # Empty scatter for legend marker
    plt.legend(loc='upper left')

    plt.title('LogP (hydrophobiciy) for each four clusters')
    plt.show()


def Stereocenter_percentile(data):
    '''
    function to visualize the dstribution of stereocenter percentile according to the drugbank

    parameter:
    data(Dataframe): dataframe with stereocenter_percentile and cluster column

    '''
    sns.boxplot(data = data, x= 'Cluster', y= 'stereo_centers_drugbank_approved_percentile', fill = False)
    medians = data.groupby('Cluster')['stereo_centers_drugbank_approved_percentile'].median()
    for i, median in enumerate(medians):
        plt.text(i, median + 0.1, f'{median:.2f}', color = 'black')

    plt.scatter([], [], color='yellow', label='Median value', marker='o')  # Empty scatter for legend marker
    plt.legend(loc='upper left')

    plt.title('stereo_centers_drugbank_approved_percentile  for each four clusters')
    plt.show()

def stereo(data):
    '''
    function to visualize the dstribution of stereocenter between the clusters

    parameter:
    data(Dataframe): dataframe with the number of sterocenter and cluster column

    '''
    sns.boxplot(data=data, x= 'Cluster', y= 'stereo_centers', fill= False)
    medians = data.groupby('Cluster')['stereo_centers'].median()
    for i, median in enumerate(medians):
        plt.text(i, median + 0.1, f'{median:.2f}', color = 'black')

    plt.scatter([], [], color='yellow', label='Median value', marker='o')  # Empty scatter for legend marker
    plt.legend(loc='upper left')

    plt.title('number of stereocenters for each four clusters')
    plt.show()


def weight(data):
    '''
    function to find the weight refleting the importance of the four properties Lipinski_score, Ki, Solubility and toxicity in a drug discovery

    parameter:
    data: dataframe containing the properties  
    '''
    data = {'Lipinski_Score': data['Cluster_1']['Lipinski'], 
                'Ki':data['Cluster_1']['Ki'], 
                'Solubility': data['Cluster_1']['Solubility_AqSolDB'],
                'Toxicity': data['Cluster_1']['ClinTox']
        }

    df = pd.DataFrame(data)

    # Standardize the dataset
    scaler = StandardScaler()
    standardized_data = scaler.fit_transform(df)

    # Perform PCA
    pca = PCA()
    pca.fit(standardized_data)

    # Calculate weights based on explained variance ratio
    weights = pca.explained_variance_ratio_
    weights_dict = {col: weight for col, weight in zip(df.columns, weights)}


    return weights_dict



def top10_compound(data, weights):
    '''
    function that selects the top 10 drugs according to 4 parameters: Lipinski_score, Ki, Solubility and Toxicity

    parameter:
    data(Dataframe): dataframe with columns Lipinski, Ki, Solubility_AqSolDB, and CLinTox
    '''

    scaler = MinMaxScaler(data)
    data = {'Lipinski_Score': data['Cluster_1']['Lipinski'], 
            'Ki':data['Cluster_1']['Ki'], 
            'Solubility': data['Cluster_1']['Solubility_AqSolDB'],
            'Toxicity': data['Cluster_1']['ClinTox']
    }

    df = pd.DataFrame(data)
    scaler = MinMaxScaler()

    # Columns to scale: Lipinski_Score (higher better), Ki (lower better), CYP450 (lower better), Solubility (moderate better)
    df[['Lipinski_Score', 'Ki', 'Solubility', 'Toxicity']] = scaler.fit_transform(
        df[['Lipinski_Score', 'Ki', 'Solubility', 'Toxicity']]
    )

    # Adjust for preference (maximize Lipinski, minimize Ki, minimize CYP450, moderate solubility)
    df['Normalized_Ki'] = 1 - df['Ki']  # Lower Ki is better

    # Step 2: Define a composite score with weights for each parameter

    # Calculate the composite score
    df['Composite_Score'] = (
        df['Lipinski_Score'] * weights['Lipinski_Score'] +
        df['Normalized_Ki'] * weights['Ki'] +
        df['Solubility'] * weights['Solubility']+
        df['Toxicity'] * weights['Toxicity']
          )

    top_10 = df.sort_values(by='Composite_Score', ascending=False).head(10)

    print("Indices of the top 10 most promising drugs:", top_10.index.tolist())
    print("Top 10 drugs data:")
    print(top_10)
    return top_10



