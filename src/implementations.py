import numpy as np
import pandas as pd

from scipy.stats import entropy

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator
from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
import plotly.graph_objects as go

from sklearn.manifold import TSNE, trustworthiness
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score

from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Descriptors
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator

from transformers import AutoTokenizer, AutoModel
import torch
import umap


def generate_fingerprints(smiles_list, radius=3, n_bits=1024):
    """
    Generates Morgan fingerprints for a list of SMILES strings.

    Morgan fingerprints are a type of circular fingerprint used in cheminformatics 
    to represent molecular structures. They are generated by considering the 
    neighborhood of atoms within a given radius and creating a binary or integer 
    vector that encodes the presence or absence of specific substructures. The 
    radius defines the size of the neighborhood, and the number of bits determines 
    the length of the resulting fingerprint vector.

    Parameters:
    smiles_list (list of str): List of SMILES strings for which to generate fingerprints.
    radius (int, optional): Radius of the circular substructures used in the Morgan fingerprint algorithm. Default is 2.
    n_bits (int, optional): Number of bits in the fingerprint vector. Default is 1024.

    Returns:
    numpy.ndarray: Array of fingerprint vectors, where each vector corresponds to a SMILES string. 
                   Entries with invalid SMILES strings are omitted.
    """
    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)
    fingerprints = []
    
    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)
        if mol:
            fp = generator.GetFingerprint(mol)
            arr = np.zeros((1,), dtype=int)
            AllChem.DataStructs.ConvertToNumpyArray(fp, arr)
            fingerprints.append(arr)
        else:
            fingerprints.append(None)
    
    return np.array([fp for fp in fingerprints if fp is not None])


def get_BERT_model(model_name):
    """
    Loads a pre-trained BERT model and its corresponding tokenizer.

    BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based
    model developed by Google for natural language understanding tasks. It is pre-trained on a 
    large corpus of text using a masked language modeling task, allowing it to capture bidirectional 
    context in a sentence. BERT can be fine-tuned for specific downstream tasks, such as text classification, 
    named entity recognition, question answering, and more.

    This function loads both the pre-trained BERT model and its tokenizer from the Hugging Face 
    Model Hub using the provided model name. The tokenizer is used to convert input text into a format 
    that the BERT model can understand, while the model is used to generate embeddings or predictions 
    based on input text.

    Parameters:
    model_name (str): The name of the pre-trained BERT model to load (e.g., 'bert-base-uncased', 'bert-large-uncased').

    Returns:
    model (transformers.PreTrainedModel): The pre-trained BERT model.
    tokenizer (transformers.PreTrainedTokenizer): The corresponding tokenizer for the BERT model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    return model, tokenizer



def plot_elbow_curve(
    method, param1_list, param2_list, Ks, 
    original_data, param1_name='n_neighbors', 
    param2_name='min_dist', metric='euclidean',generate_MF=False):
    """
    Function to plot elbow curves of dimension reduced data, for clustering analysis on embeddings.

    Parameters:
        method (str): Dimensionality reduction method ('umap' or 'tsne').
        original_data (array): Original embeddings for UMAP, SMILES list for t-SNE.
        param1_list (list): List of values for the first parameter (e.g., 'n_neighbors' or 'radius').
        param2_list (list): List of values for the second parameter (e.g., 'min_dist' or 'perplexity').
        Ks (range): Range of k values for KMeans clustering.
        param1_name (str): Name of the first parameter.
        param2_name (str): Name of the second parameter.
        metric (str): Distance metric to use.

    Returns:
        tuple: results (list), scores (array), silhouette_scores (list)
    """
    
    nrows, ncols = len(param1_list), len(param2_list)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4*ncols, 4*nrows))
    axes = np.atleast_2d(axes)
    sns.set_palette('deep', n_colors=nrows * ncols)
    
    results = [[[] for _ in range(ncols)] for _ in range(nrows)]
    scores = np.zeros((nrows, ncols))
    silhouette_scores = [[[] for _ in range(ncols)] for _ in range(nrows)]
    
    for idx1, param1 in enumerate(param1_list):
        print(f'{param1_name.capitalize()} = {param1}')
        
        if generate_MF:
            embeddings = generate_fingerprints(original_data, radius=param1)
        else:
            embeddings = original_data
        
        for idx2, param2 in enumerate(param2_list):
            print(f'{param2_name.capitalize()} = {param2}')
            ax = axes[idx1][idx2]
            
            if method == 'umap':
                model = umap.UMAP(n_components=3, n_neighbors=param1, min_dist=param2, metric=metric)
                result = model.fit_transform(embeddings)
                scores[idx1][idx2] = trustworthiness(embeddings, result, n_neighbors=param1)
                
            elif method == 'tsne':
                model = TSNE(n_components=3, perplexity=param2, init='random', learning_rate='auto', random_state=0, metric=metric)
                result = model.fit_transform(embeddings)
                scores[idx1][idx2] = model.kl_divergence_
            
            results[idx1][idx2][:] = result
            
            inertia = []
            s_score = []
            for k in Ks:
                kmeans = KMeans(n_clusters=k, random_state=42)
                kmeans.fit(result)
                inertia.append(kmeans.inertia_)
                s_score.append(silhouette_score(result, kmeans.labels_))
            silhouette_scores[idx1][idx2][:] = s_score
            
            ax.plot(Ks, inertia, 'o-', markersize=8, label=f'{param1_name}={param1}, {param2_name}={param2}', color=np.random.rand(3,))
            ax.xaxis.set_major_locator(MaxNLocator(integer=True))
            ax.legend()
            ax.set_xlabel('Number of Clusters (k)')
            ax.set_ylabel('Inertia')
            ax.grid(True)

    plt.suptitle(f'Elbow Method for Optimal k with varying {param1_name} and {param2_name}', fontweight='bold', fontsize=12)
    plt.tight_layout()
    plt.show()

    return results, scores, silhouette_scores


def plot_silhouette_scores(
    s_scores, Ks, param1_values, param2_values, param1_name, param2_name, 
    y_lim, color_palette="inferno", title="silhouette score for different k"
):
    """
    Function to plot silhouette scores for clustering analysis.

    Parameters:
        s_scores (list): A nested list of silhouette scores for each combination of parameters.
        Ks (range): Range of k values for KMeans clustering.
        param1_values (list): List of values for the first parameter (e.g., 'n_neighbors' or 'radius').
        param2_values (list): List of values for the second parameter (e.g., 'min_dist' or 'perplexity').
        param1_name (str): Name of the first parameter to display in the title (e.g., 'NN' or 'r').
        param2_name (str): Name of the second parameter to display in the legend (e.g., 'd' or 'p').
        y_lim (tuple): Y-axis limits for the silhouette scores.
        color_palette (str): Color palette for plotting.
        title (str): Title for the overall plot.
    """
    ncols = len(param1_values)
    fig, axes = plt.subplots(nrows=1, ncols=ncols, figsize=(4*ncols, 6))
    axes = np.atleast_2d(axes)
    axes = axes.flatten()
    palette = sns.color_palette(color_palette, n_colors=len(param2_values))
    
    for idx1, row in enumerate(s_scores):
        for idx2, s_score in enumerate(row):
            axes[idx1].plot(Ks, s_score, label=f'{param2_name}={param2_values[idx2]}', color=palette[idx2])
        axes[idx1].legend()
        axes[idx1].xaxis.set_major_locator(MaxNLocator(integer=True))
        axes[idx1].set_xlabel('number of clusters (k)')
        axes[idx1].set_ylabel('silhouette score')
        axes[idx1].set_ylim(y_lim)
        axes[idx1].set_title(f'{param1_name}={param1_values[idx1]}')
        axes[idx1].grid(True)

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()


def normalized_entropy(counts):
    """
    Calculates the normalized entropy of a discrete distribution.

    Entropy is a measure of uncertainty or unpredictability in a distribution.
    The normalized entropy scales the raw entropy value so that it lies between 0 and 1.
    A normalized entropy of 1 corresponds to the highest possible entropy (maximum uncertainty), 
    which occurs when the distribution is uniform (all categories are equally likely). 
    A normalized entropy of 0 corresponds to the lowest possible entropy (minimum uncertainty), 
    which occurs when all counts are concentrated in a single category.

    The raw entropy is calculated using the Shannon entropy formula, and then it is normalized 
    by dividing it by the maximum possible entropy, which for a discrete distribution is the 
    logarithm of the number of categories (k).

    Parameters:
    counts (array-like): Array of counts for each category in the distribution.
                          Each entry represents the number of occurrences of a particular category.

    Returns:
    float: Normalized entropy value between 0 and 1. 
           - 1 indicates maximum entropy (uniform distribution, highest uncertainty).
           - 0 indicates minimum entropy (distribution concentrated in a single category).
    """
    k = len(counts) 
    probs = counts / counts.sum()  
    raw_entropy = entropy(probs)   
    max_entropy = np.log(k)       
    return raw_entropy / max_entropy 



def plot_cluster_distributions(
    results, best_ks, param1_values, param2_values, param1_name, param2_name,
    palette='viridis', figsize=(25, 25), title="Clusters distribution"
):
    """
    Function to plot the distribution of clusters for different combinations of parameters.
    Uses normalized entropy as indicator metric.

    Parameters:
        results (list): List of dimensionality-reduced results for each combination of parameters.
        best_ks (list): List of best k values for each parameter combination.
        param1_values (list): List of values for the first parameter (e.g., radiuses or n_neighbors).
        param2_values (list): List of values for the second parameter (e.g., perplexities or min_dists).
        param1_name (str): Name of the first parameter to display in the title.
        param2_name (str): Name of the second parameter to display in the title.
        palette (str): Color palette for plotting.
        figsize (tuple): Figure size for the plot.
        title (str): Title for the overall plot.
    """
    nrows, ncols = len(param1_values), len(param2_values)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
    axes = np.atleast_2d(axes)
    
    for idx1, param1 in enumerate(param1_values):
        for idx2, param2 in enumerate(param2_values):
            ax = axes[idx1][idx2]
            kmeans = KMeans(n_clusters=best_ks[idx1][idx2], random_state=42)
            kmeans.fit(results[idx1][idx2])

            # Add cluster labels to the DataFrame
            labels = pd.DataFrame(kmeans.labels_, columns=['labels'])
            cluster_counts = labels['labels'].value_counts()
            entropy_score = round(normalized_entropy(cluster_counts), 4)

            sns.countplot(x='labels', hue='labels', data=labels, palette=palette, ax=ax)
            ax.set_title(f'({param1_name}={param1} | {param2_name}={param2} | entropy score={entropy_score})')
            ax.set_xlabel('Cluster')
            ax.set_ylabel('Number of Ligands')

            legend_labels = [f'{cluster} ({cluster_counts.get(cluster, 0)})' for cluster in range(best_ks[idx1][idx2])]
            handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='grey', markersize=10) for _ in range(best_ks[idx1][idx2])]
            ax.legend(handles, legend_labels, title='Cluster')

    plt.suptitle(title, fontweight='bold', fontsize=18, y=1.001)
    plt.tight_layout()
    plt.show()


def plot_3d_clusters(results, min_index, k, method_name):
    """
    Function to create a 3D interactive scatter plot with clusters based on dimensionality reduction results.

    Parameters:
        results (list): List of dimensionality-reduced results for each parameter combination.
        min_index (tuple): Index of the optimal parameters in the results list.
        k (int): Number of clusters for KMeans.
        method_name (str): Name of the method (e.g., 'TSNE' or 'UMAP') for labeling axes.
    """
    # KMeans clustering on the selected result
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(results[min_index[0]][min_index[1]])
    df = pd.DataFrame(results[min_index[0]][min_index[1]], columns=[f'{method_name}1', f'{method_name}2', f'{method_name}3'])
    df['Cluster'] = kmeans.labels_

    # Create 3D scatter plot
    fig = go.Figure()
    scatter = go.Scatter3d(
        x=df[f'{method_name}1'],
        y=df[f'{method_name}2'],
        z=df[f'{method_name}3'],
        mode='markers',
        marker=dict(size=5, color=df['Cluster'], colorscale='Inferno', opacity=0.7)
    )
    fig.add_trace(scatter)

    # Customize layout
    fig.update_layout(
        title=f"{method_name} Clusters",
        scene=dict(
            xaxis_title=f'{method_name}1',
            yaxis_title=f'{method_name}2',
            zaxis_title=f'{method_name}3',
            xaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
            yaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
            zaxis=dict(showgrid=True, gridwidth=2, gridcolor='gray'),
        ),
        height=800, width=1200,
        template="plotly_white"
    )

    # Display the figure
    fig.show()

    return df, fig
